# -*- coding: utf-8 -*-
"""neuro240.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LIoSQXtDa88gpmbOgD5P_8BAyttbXdTR

## Installs
"""

!pip install datasets openai

!pip install ace_tools

!pip install faiss-cpu

"""## Setup + Load Data"""

import os
import random
import numpy as np
import torch
import networkx as nx
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from tqdm import tqdm
from huggingface_hub import login
from openai import OpenAI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer, util
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Enable CUDA (GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Set Hugging Face authentication token
login(token=os.environ.get("HF_TOKEN"))  # Set HF_TOKEN in your .env file

# Initialize OpenAI client
openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))  # Set OPENAI_API_KEY in your .env file
openai_model = "gpt-4o-mini"

# Initialize embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Set seed for reproducibility
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# Switch to a lightweight model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add reasoning tokens
reasoning_tokens = ["<think>", "</think>", "<verify>", "</verify>", "<conclude>", "</conclude>"]
tokenizer.add_tokens(reasoning_tokens)
model.resize_token_embeddings(len(tokenizer))

# Set pad token (some models don't have one by default)
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.eos_token_id

model.to(device)

# Load HLE dataset subset
def load_hle_subset(num_samples=50):
    try:
        dataset = load_dataset("cais/hle", split="test")
        if len(dataset) > num_samples:
            subset_indices = random.sample(range(len(dataset)), num_samples)
            dataset = dataset.select(subset_indices)
        return dataset
    except Exception as e:
        print(f"Error loading HLE dataset: {e}")
        return [{"question": f"Question {i}: What is 2x + 3 = 7?", "answer": "x = 2", "answer_type": "exactMatch"} for i in range(num_samples)]

hle_dataset = load_hle_subset(50)
print(f"Loaded {len(hle_dataset)} HLE questions")

# Enforce structured reasoning token usage
def prepare_prompt(question):
    return (
        f"You should answer the following question in a structured analytical manner. Follow these rules:\n"
        f"1. Inside <think> </think> tags, break down the question into key concepts and reasoning steps.\n"
        f"2. Inside <verify> </verify> tags, verify the logic by checking calculations, consistency, or common mistakes.\n"
        f"3. Inside <conclude> </conclude> tags, ONLY state the final answer. Do not include explanations here.\n"
        f"\nQuestion: {question}\n"
    )

class ExtractedResponse(BaseModel):
    thinking: str
    verification: str
    conclusion: str
    raw_text: str

"""## Multiple Models"""

def setup_model(model_name, device="cuda" if torch.cuda.is_available() else "cpu"):
    """
    Set up a lightweight model with all necessary configurations.

    Args:
        model_name: Name of the model from HuggingFace
        device: Device to load the model on ("cuda", "cpu", etc.)

    Returns:
        tuple: (tokenizer, model)
    """
    print(f"Loading {model_name} on {device}...")

    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    # Add reasoning tokens
    reasoning_tokens = ["<think>", "</think>", "<verify>", "</verify>", "<conclude>", "</conclude>"]
    tokenizer.add_tokens(reasoning_tokens)
    model.resize_token_embeddings(len(tokenizer))

    # Set pad token if it doesn't exist
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.eos_token_id

    # Move model to the specified device
    model.to(device)

    print(f"Successfully loaded {model_name}")
    return tokenizer, model

# List of lightweight models to choose from
LIGHTWEIGHT_MODELS = {
    "tinyllama": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "phi": "microsoft/phi-2",
    "stablelm": "stabilityai/stablelm-3b-4e1t",
    "flan_t5_small": "google/flan-t5-small",
    "gpt2": "gpt2",
    "opt": "facebook/opt-1.3b",
    "pythia": "EleutherAI/pythia-1.4b",
}

selected_model = "phi"

# Setup the model
tokenizer, model = setup_model(LIGHTWEIGHT_MODELS[selected_model])

"""## Generate Text + Reward + Evaluate

### old code
"""

# # Use TinyLlama to generate the response and OpenAI to extract structured information
# def generate_text(model, tokenizer, question, answer_type, max_length=150):
#     model.eval()
#     prompt = prepare_prompt(question)

#     encoded = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
#     input_ids = encoded.input_ids

#     with torch.no_grad():
#         outputs = model.generate(
#             input_ids=input_ids,
#             max_new_tokens=max_length,
#             pad_token_id=tokenizer.pad_token_id,
#             temperature=0.7,
#             do_sample=True,
#             num_return_sequences=1
#         )

#     tinyllama_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

#     system_prompt = """You are a structured reasoning assistant. Extract the following:
#     - thinking: The reasoning process used to arrive at the answer
#     - verification: The steps or checks performed to confirm the answer
#     - conclusion: The final answer (only the answer, no explanation)
#     """

#     response = openai_client.beta.chat.completions.parse(
#         model=openai_model,
#         messages=[
#             {"role": "system", "content": system_prompt},
#             {"role": "user", "content": f"Extract components from:\n{tinyllama_output}"}
#         ],
#         response_format=ExtractedResponse,
#         temperature=0,
#         max_tokens=900
#     )

#     result = response.choices[0].message.parsed
#     result.raw_text = tinyllama_output
#     return result.dict()

# # Compute embedding similarity for exactMatch answers
# def compute_embedding_similarity(answer1, answer2):
#     emb1 = embedding_model.encode(answer1, convert_to_tensor=True)
#     emb2 = embedding_model.encode(answer2, convert_to_tensor=True)
#     return util.pytorch_cos_sim(emb1, emb2).item()

# # Reward function
# def compute_reward(question, generated_output, correct_answer, answer_type):
#     extracted_answer = generated_output.get("conclusion", "").strip()
#     lambda_consistency = 0.4
#     lambda_stepwise = 0.3
#     lambda_answer = 0.3

#     has_thinking = "thinking" in generated_output and generated_output["thinking"]
#     stepwise_score = 0.5 if has_thinking else 0.0

#     if answer_type == "exactMatch":
#         answer_score = compute_embedding_similarity(extracted_answer, correct_answer)
#     elif answer_type == "multipleChoice" and extracted_answer.upper() == correct_answer.upper():
#         answer_score = 1.0
#     else:
#         answer_score = 0.0

#     reward = (lambda_consistency + lambda_stepwise * stepwise_score + lambda_answer * answer_score)
#     print(f"\nQuestion: {question}")
#     print(f"Generated Output: {generated_output}")
#     print(f"Reward: {reward:.4f}")
#     return max(min(reward, 1.5), -1.0)

# # Evaluation function
# def evaluate_model(model, tokenizer, eval_dataset):
#     model.eval()
#     rewards = []
#     for item in eval_dataset:
#         question = item["question"]
#         answer = item["answer"]
#         answer_type = item["answer_type"]
#         generated_output = generate_text(model, tokenizer, question, answer_type)
#         reward = compute_reward(question, generated_output, answer, answer_type)
#         rewards.append(reward)
#     avg_reward = sum(rewards) / len(rewards)
#     print(f"Average reward: {avg_reward:.4f}")
#     return avg_reward

# # Final evaluation
# print("\n--- Evaluating baseline model ---")
# baseline_reward = evaluate_model(model, tokenizer, hle_dataset)

# # Train model (training logic remains the same)

# # Final evaluation after fine-tuning
# print("\n--- Evaluating fine-tuned model ---")
# final_reward = evaluate_model(model, tokenizer, hle_dataset)
# print(f"Performance Improvement: {final_reward - baseline_reward:.4f}")

# # Use TinyLlama to generate the response and OpenAI to extract structured information
# def generate_text(model, tokenizer, question, answer_type, max_length=150):
#     model.eval()
#     prompt = prepare_prompt(question)

#     encoded = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
#     input_ids = encoded.input_ids

#     with torch.no_grad():
#         outputs = model.generate(
#             input_ids=input_ids,
#             max_new_tokens=max_length,
#             pad_token_id=tokenizer.pad_token_id,
#             temperature=0.7,
#             do_sample=True,
#             num_return_sequences=1
#         )

#     tinyllama_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

#     system_prompt = """You are a structured reasoning assistant. Extract the following:
#     - thinking: The reasoning process used to arrive at the answer
#     - verification: The steps or checks performed to confirm the answer
#     - conclusion: The final answer (only the answer, no explanation)
#     """

#     response = openai_client.beta.chat.completions.parse(
#         model=openai_model,
#         messages=[
#             {"role": "system", "content": system_prompt},
#             {"role": "user", "content": f"Extract components from:\n{tinyllama_output}"}
#         ],
#         response_format=ExtractedResponse,
#         temperature=0,
#         max_tokens=900
#     )

#     result = response.choices[0].message.parsed
#     result.raw_text = tinyllama_output
#     return result.dict()

# # Compute embedding similarity for exactMatch answers
# def compute_embedding_similarity(answer1, answer2):
#     emb1 = embedding_model.encode(answer1, convert_to_tensor=True)
#     emb2 = embedding_model.encode(answer2, convert_to_tensor=True)
#     return util.pytorch_cos_sim(emb1, emb2).item()

# # Reward function
# def compute_reward(question, generated_output, correct_answer, answer_type):
#     extracted_answer = generated_output.get("conclusion", "").strip()
#     lambda_consistency = 0.4
#     lambda_stepwise = 0.3
#     lambda_answer = 0.3

#     has_thinking = "thinking" in generated_output and generated_output["thinking"]
#     stepwise_score = 0.5 if has_thinking else 0.0

#     if answer_type == "exactMatch":
#         answer_score = compute_embedding_similarity(extracted_answer, correct_answer)
#     elif answer_type == "multipleChoice" and extracted_answer.upper() == correct_answer.upper():
#         answer_score = 1.0
#     else:
#         answer_score = 0.0

#     reward = (lambda_consistency + lambda_stepwise * stepwise_score + lambda_answer * answer_score)
#     print(f"\nQuestion: {question}")
#     print(f"Generated Output: {generated_output}")
#     print(f"Reward: {reward:.4f}")
#     return max(min(reward, 1.5), -1.0)

# # Evaluation function
# def evaluate_model(model, tokenizer, eval_dataset):
#     model.eval()
#     rewards = []
#     for item in eval_dataset:
#         question = item["question"]
#         answer = item["answer"]
#         answer_type = item["answer_type"]
#         generated_output = generate_text(model, tokenizer, question, answer_type)
#         reward = compute_reward(question, generated_output, answer, answer_type)
#         rewards.append(reward)
#     avg_reward = sum(rewards) / len(rewards)
#     print(f"Average reward: {avg_reward:.4f}")
#     return avg_reward

# def evaluate_logical_consistency(graph):
#     """Evaluate logical consistency based on graph structure."""
#     # Check if the graph is a valid directed acyclic graph (DAG)
#     if not nx.is_directed_acyclic_graph(graph):
#         return -0.5  # Penalize cycles in reasoning

#     # Check connectivity between nodes
#     if len(graph.nodes) > 0 and not nx.is_weakly_connected(graph):
#         return 0.0  # Neutral score for disconnected reasoning

#     # Calculate score based on graph properties
#     num_nodes = len(graph.nodes)
#     if num_nodes <= 1:
#         return 0.2  # Basic reasoning present

#     # Higher score for more structured reasoning
#     return min(0.8, 0.2 + 0.1 * num_nodes)

# def generate_text(model, tokenizer, question, answer_type, max_length=150):
#     """Generate text using TinyLlama and extract structured components with OpenAI."""
#     model.eval()
#     prompt = prepare_prompt(question)

#     encoded = tokenizer(
#         prompt, return_tensors="pt", truncation=True, max_length=512
#     ).to(device)
#     input_ids = encoded.input_ids

#     with torch.no_grad():
#         outputs = model.generate(
#             input_ids=input_ids,
#             max_new_tokens=max_length,
#             pad_token_id=tokenizer.pad_token_id,
#             temperature=0.7,
#             do_sample=True,
#             num_return_sequences=1,
#         )

#     tinyllama_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

#     system_prompt = """You are a structured reasoning assistant. Extract the following:
#     - thinking: The reasoning process used to arrive at the answer
#     - verification: The steps or checks performed to confirm the answer
#     - conclusion: The final answer (only the answer, no explanation)
#     """

#     response = openai_client.beta.chat.completions.parse(
#         model=openai_model,
#         messages=[
#             {"role": "system", "content": system_prompt},
#             {
#                 "role": "user",
#                 "content": f"Extract components from:\n{tinyllama_output}",
#             },
#         ],
#         response_format=ExtractedResponse,
#         temperature=0,
#         # max_tokens=900,
#         # max_tokens=12000
#     )

#     result = response.choices[0].message.parsed
#     result.raw_text = tinyllama_output
#     return result.dict()

# def evaluate_model(model, tokenizer, eval_dataset):
#     """Evaluate model performance on a dataset."""
#     model.eval()
#     rewards = []
#     correctness_scores = []
#     total_questions = len(eval_dataset)

#     for item in eval_dataset:
#         question = item["question"]
#         answer = item["answer"]
#         answer_type = item["answer_type"]

#         generated_output = generate_text(model, tokenizer, question, answer_type)

#         # Get answer correctness score
#         extracted_answer = generated_output.get("conclusion", "").strip()
#         answer_correctness_score = evaluate_answer_correctness(
#             extracted_answer, answer, answer_type, question
#         )

#         # Calculate reward
#         reward = compute_reward(question, generated_output, answer, answer_type)

#         # Record metrics
#         rewards.append(reward)
#         correctness_scores.append(answer_correctness_score)

#     # Calculate averages
#     avg_reward = sum(rewards) / len(rewards) if rewards else 0
#     avg_accuracy = sum(correctness_scores) / len(correctness_scores) if correctness_scores else 0

#     # Also report the number of answers above threshold for reference
#     threshold = 0.7  # You can adjust this
#     correct_answers = sum(1 for score in correctness_scores if score >= threshold)

#     print(f"Average reward: {avg_reward:.4f}")
#     print(f"Average accuracy: {avg_accuracy:.4f}")
#     print(f"Binary accuracy @ {threshold}: {correct_answers}/{total_questions} ({correct_answers/total_questions:.4f})")

#     return avg_reward, avg_accuracy

# # PPO training implementation
# def train_with_ppo(model, tokenizer, train_dataset, num_epochs=3, batch_size=8, learning_rate=5e-5):
#     """Train model using Proximal Policy Optimization (PPO)."""
#     print("\n--- Starting PPO Training ---")

#     # Prepare optimizer for policy model
#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

#     # Training loop
#     total_samples = 0
#     for epoch in range(num_epochs):
#         print(f"\nEpoch {epoch+1}/{num_epochs}")

#         # Get indices for this epoch
#         indices = list(range(len(train_dataset)))
#         random.shuffle(indices)

#         # Process mini-batches
#         for batch_start in range(0, len(indices), batch_size):
#             batch_indices = indices[batch_start:min(batch_start + batch_size, len(indices))]

#             # Process each question in batch
#             for idx in batch_indices:
#                 # Get item from dataset
#                 item = train_dataset[idx]

#                 question = item["question"]
#                 answer = item["answer"]
#                 answer_type = item["answer_type"]

#                 try:
#                     # Prepare the prompt
#                     prompt = prepare_prompt(question)
#                     input_text = tokenizer.encode(prompt, return_tensors="pt").to(device)

#                     # Forward pass with requires_grad=True
#                     model.train()  # Set model to training mode
#                     outputs = model(input_text)
#                     logits = outputs.logits

#                     # Generate a response for this training iteration
#                     with torch.no_grad():  # No grad for generation
#                         generated_output = generate_text(model, tokenizer, question, answer_type)

#                     # Compute reward for this response
#                     reward_value = float(compute_reward(question, generated_output, answer, answer_type))

#                     # Create a proper loss that connects to model parameters
#                     # We use a simple approach: multiply the average logit with the reward
#                     # This encourages the model to produce higher logits (more confident predictions)
#                     # when the reward is high and lower when the reward is low
#                     scale_factor = 0.01  # Scale factor to keep gradients reasonable
#                     reward_tensor = torch.tensor(reward_value, dtype=torch.float32)
#                     loss = -torch.mean(logits) * reward_tensor * scale_factor

#                     # This loss will encourage the model to be more confident in its predictions
#                     # when they lead to good rewards, and less confident when they lead to bad rewards

#                     # Backward pass and optimization
#                     optimizer.zero_grad()
#                     loss.backward()

#                     # Clip gradients to prevent large updates
#                     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
#                     optimizer.step()

#                     print(f"Processed question with reward: {reward_value:.4f}, loss: {loss.item():.6f}")
#                     total_samples += 1

#                 except Exception as e:
#                     print(f"Error processing sample: {e}")
#                     continue

#             print(f"Processed batch {batch_start+1}-{batch_start+len(batch_indices)}/{len(train_dataset)}, Total samples: {total_samples}")

#     print("--- PPO Training Complete ---")
#     return model

"""### updated implementation"""

import os
import random
import numpy as np
import torch
import networkx as nx
import re
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from openai import OpenAI
from pydantic import BaseModel
from typing import Dict, List, Any, Optional, Tuple
import faiss
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import json

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")
try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")

nltk.download('punkt_tab')

# Initialize embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")


def generate_text(model, tokenizer, question, answer_type, max_length=150):
    """Generate text using TinyLlama and extract structured components with OpenAI."""
    try:
        model.eval()
        prompt = prepare_prompt(question)

        encoded = tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=512
        ).to(device)
        input_ids = encoded.input_ids

        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                max_new_tokens=max_length,
                pad_token_id=tokenizer.pad_token_id,
                temperature=0.7,
                do_sample=True,
                num_return_sequences=1,
            )

        model_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

        system_prompt = """You are a structured reasoning assistant. Extract the following:
        - thinking: The reasoning process used to arrive at the answer
        - verification: The steps or checks performed to confirm the answer
        - conclusion: The final answer (only the answer, no explanation)
        """

        response = openai_client.beta.chat.completions.parse(
            model=openai_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": f"Extract components from:\n{model_output}",
                },
            ],
            response_format=ExtractedResponse,
            temperature=0,
        )

        result = response.choices[0].message.parsed
        result.raw_text = model_output
        return result.dict()

    except Exception as e:
        print(f"Error in generate_text: {str(e)[:100]}...")
        # Return None to indicate we should skip this question
        return None

def compute_embedding_similarity(answer1, answer2):
    """Compute embedding similarity between two text strings."""
    emb1 = embedding_model.encode(answer1, convert_to_tensor=True)
    emb2 = embedding_model.encode(answer2, convert_to_tensor=True)
    return util.pytorch_cos_sim(emb1, emb2).item()


def extract_reasoning_steps(thinking_text):
    """Extract individual reasoning steps from thinking text."""
    # Split text into sentences and filter out short sentences
    sentences = sent_tokenize(thinking_text)
    steps = [s.strip() for s in sentences if len(s.strip()) > 10]

    # Alternatively, look for numbered steps or explicit reasoning markers
    numbered_steps = re.findall(r"\d+\.\s+([^\d]+?)(?=\d+\.|$)", thinking_text)
    if numbered_steps and len(numbered_steps) > 1:
        steps = numbered_steps

    return steps


def build_reasoning_graph(thinking_steps):
    """Build a directed graph representing the reasoning flow."""
    G = nx.DiGraph()

    # Add nodes for each reasoning step
    for i, step in enumerate(thinking_steps):
        G.add_node(i, text=step)

    # Add edges between consecutive steps to represent the reasoning flow
    for i in range(len(thinking_steps) - 1):
        G.add_edge(i, i + 1)

    return G


def evaluate_logical_consistency(graph, node_contents=None, domain=None):
    """Evaluate logical consistency with improved robustness."""
    # Base case checks
    if not graph or len(graph.nodes) == 0:
        return 0.0

    # Initialize scores for different dimensions
    scores = {
        "acyclicity": 0.0,
        "connectivity": 0.0,
        "flow_quality": 0.0,
        "depth_breadth": 0.0,
        "semantic_coherence": 0.0
    }

    # 1. Improved cycle detection with graduated response
    if not nx.is_directed_acyclic_graph(graph):
        # Count cycles and penalize proportionally
        cycles = list(nx.simple_cycles(graph))
        cycle_penalty = min(0.5, 0.1 * len(cycles))
        scores["acyclicity"] = -cycle_penalty
    else:
        scores["acyclicity"] = 0.2

    # 2. Enhanced connectivity analysis
    components = list(nx.weakly_connected_components(graph))
    if len(components) > 1:
        # Smaller penalty for minor disconnections
        largest_component_size = max(len(c) for c in components)
        connectivity_ratio = largest_component_size / len(graph.nodes)
        scores["connectivity"] = 0.2 * connectivity_ratio
    else:
        scores["connectivity"] = 0.2

    # 3. Analyze reasoning flow quality
    if len(graph.nodes) > 1:
        # Examine path lengths and distributions
        longest_path_length = max(len(p) for p in nx.all_simple_paths(graph,
                                  source=list(graph.nodes)[0],
                                  target=list(nx.dag.dag_longest_path(graph))[-1])
                                 ) if nx.is_directed_acyclic_graph(graph) else 1

        # Reward deeper reasoning chains
        scores["flow_quality"] = min(0.2, 0.05 * longest_path_length)

        # 4. Assess depth vs breadth balance
        avg_in_degree = sum(d for _, d in graph.in_degree()) / len(graph.nodes)
        avg_out_degree = sum(d for _, d in graph.out_degree()) / len(graph.nodes)
        balance_score = min(avg_in_degree, avg_out_degree) / max(1, max(avg_in_degree, avg_out_degree))
        scores["depth_breadth"] = 0.2 * balance_score

    # 5. Semantic coherence if node contents provided
    if node_contents:
        coherence_score = 0.0
        edge_semantic_scores = []
        contradiction_penalty = 0.0
        inference_quality_scores = []

        # Check each edge for semantic coherence
        for source, target in graph.edges():
            if source in node_contents and target in node_contents:
                source_content = node_contents[source]
                target_content = node_contents[target]

                # 1. Check for semantic similarity between connected nodes
                # This would ideally use embeddings or NLI models
                # For now, we'll implement a simplified version

                # Extract key terms from both nodes
                source_terms = set(source_content.lower().split())
                target_terms = set(target_content.lower().split())

                # Calculate lexical overlap
                overlap = len(source_terms.intersection(target_terms))
                total_terms = len(source_terms.union(target_terms))
                overlap_score = overlap / max(1, total_terms)

                # 2. Check for logical connectors in target node
                logical_connectors = ["therefore", "thus", "hence", "so", "because",
                                     "since", "as a result", "consequently"]
                has_connectors = any(connector in target_content.lower() for connector in logical_connectors)
                connector_score = 0.2 if has_connectors else 0.0

                # 3. Check for contradictions (simplified)
                contradiction_indicators = ["however", "but", "although", "despite",
                                           "nevertheless", "conversely", "on the contrary"]
                contradiction_signals = sum(1 for ind in contradiction_indicators if ind in target_content.lower())
                if contradiction_signals > 1:
                    contradiction_penalty += 0.1 * min(2, contradiction_signals)

                # 4. Check if conclusion words appear in target
                conclusion_indicators = ["conclude", "conclusion", "summary", "finally",
                                        "in summary", "ultimately", "in conclusion"]
                is_conclusion = any(ind in target_content.lower() for ind in conclusion_indicators)

                # Calculate edge coherence score
                edge_score = 0.5 * overlap_score + 0.3 * connector_score + 0.2 * (1.0 if is_conclusion else 0.0)
                edge_semantic_scores.append(edge_score)

                # If it's a domain-specific evaluation, check domain-relevant terms
                if domain:
                    domain_terms = domain_term_library.get(domain, [])  # Assuming domain_term_library exists
                    domain_term_usage = sum(1 for term in domain_terms if term in target_content.lower())
                    domain_bonus = min(0.2, 0.02 * domain_term_usage)
                    inference_quality_scores.append(domain_bonus)

        # Calculate overall semantic coherence
        if edge_semantic_scores:
            average_edge_score = sum(edge_semantic_scores) / len(edge_semantic_scores)
            coherence_score = average_edge_score - contradiction_penalty

            # Add inference quality if domain-specific evaluation was done
            if inference_quality_scores:
                coherence_score += sum(inference_quality_scores) / len(inference_quality_scores)

            # Normalize and cap
            scores["semantic_coherence"] = max(0.0, min(0.2, coherence_score))
        else:
            scores["semantic_coherence"] = 0.0

    # Calculate final weighted score
    weights = {
        "acyclicity": 0.3,
        "connectivity": 0.2,
        "flow_quality": 0.2,
        "depth_breadth": 0.2,
        "semantic_coherence": 0.1 if node_contents else 0.0
    }

    # Normalize weights if semantic_coherence is not used
    if not node_contents:
        weights = {k: v / sum(weights.values()) for k, v in weights.items()}

    final_score = sum(scores[k] * weights[k] for k in scores)

    # Cap final score between -0.5 and 1.0
    return max(-0.5, min(1.0, final_score))

def evaluate_stepwise_correctness(thinking_steps, verification_text):
    """Evaluate the correctness of reasoning steps."""
    if not thinking_steps:
        return 0.0

    # Basic check: verify that verification references the thinking steps
    step_references = 0
    for step in thinking_steps:
        # Create a simplified version of the step for comparison
        step_keywords = set(
            word.lower()
            for word in re.findall(r"\b\w+\b", step)
            if word.lower() not in stopwords.words("english") and len(word) > 3
        )

        # Check if verification text contains keywords from this step
        verification_words = set(
            word.lower()
            for word in re.findall(r"\b\w+\b", verification_text)
            if len(word) > 3
        )

        common_keywords = step_keywords.intersection(verification_words)
        if len(common_keywords) >= min(2, len(step_keywords) // 3):
            step_references += 1

    # Calculate score based on the portion of steps that were verified
    if len(thinking_steps) > 0:
        return min(0.9, 0.3 + 0.6 * (step_references / len(thinking_steps)))
    return 0.3  # Base score if steps exist but none are verified


def detect_hallucinations(reasoning_text, question, threshold=0.4):
    """Enhanced heuristic for detecting potential hallucinations."""
    if not reasoning_text:
        return 0.0

    # 1. Pattern-based detection with expanded patterns
    hallucination_patterns = [
        # Overconfidence patterns
        r"it is (well-known|widely accepted|obviously|clearly|certainly|definitely) that",
        r"(everyone|all experts|all scientists|all mathematicians) (knows|agree|accept)",
        r"(without a doubt|undoubtedly|unquestionably|absolutely|certainly)",
        r"(the only possible|the sole|the exclusive|the definitive) (answer|solution|approach)",

        # Citation hallucinations
        r"(according to|as stated in|as shown in) (research|studies|papers|experiments|literature)",
        r"(studies|research|papers|experts|scientists) (have shown|have proven|have demonstrated|have found)",

        # Specific claim patterns
        r"(exactly|precisely) (the same as|equal to|equivalent to)",
        r"(always|never|in all cases|in every instance|without exception)",

        # Number hallucinations
        r"(\d+) (percent|per cent|%)",
        r"in (\d{4}), (.*) (discovered|invented|found|proven)",
    ]

    # 2. Calculate relevance of reasoning to question
    question_embedding = embedding_model.encode(question, convert_to_tensor=True)
    reasoning_embedding = embedding_model.encode(reasoning_text, convert_to_tensor=True)
    relevance_score = util.pytorch_cos_sim(question_embedding, reasoning_embedding).item()

    # If reasoning seems irrelevant to question, increase hallucination score
    irrelevance_penalty = max(0, threshold - relevance_score) * 2.0

    # 3. Extract statements and check each one
    sentences = sent_tokenize(reasoning_text)
    statements_score = 0.0

    # Set of key terms from question to check against
    question_words = set(re.findall(r'\b[A-Za-z]{4,}\b', question.lower()))

    for sentence in sentences:
        # Skip short sentences
        if len(sentence.split()) < 4:
            continue

        # Check for specific patterns
        pattern_detected = False
        for pattern in hallucination_patterns:
            if re.search(pattern, sentence.lower()):
                statements_score += 0.15
                pattern_detected = True
                break

        # If no pattern detected, check for statement disconnection
        if not pattern_detected:
            # Look for sentences that introduce new concepts not in question
            sentence_words = set(re.findall(r'\b[A-Za-z]{4,}\b', sentence.lower()))
            new_terms = sentence_words - question_words

            # If sentence introduces many new terms AND uses strong language
            if len(new_terms) > 3 and any(intensifier in sentence.lower() for intensifier in
                                          ["very", "extremely", "highly", "completely", "truly"]):
                statements_score += 0.1

    # 4. Check for numerical consistency
    numbers_in_question = set(re.findall(r'\b\d+\b', question))
    numbers_in_reasoning = set(re.findall(r'\b\d+\b', reasoning_text))
    new_numbers = numbers_in_reasoning - numbers_in_question

    # If reasoning introduces many new numbers not in question
    numbers_penalty = min(0.2, len(new_numbers) * 0.05)

    # 5. Final hallucination score calculation
    pattern_score = min(0.5, statements_score)
    hallucination_score = pattern_score + irrelevance_penalty + numbers_penalty

    # Cap and return the hallucination penalty
    return min(0.8, hallucination_score)


def evaluate_answer_correctness(extracted_answer, correct_answer, answer_type, question):
    """Evaluate answer correctness using an LLM-based approach with nuanced scoring."""
    if not extracted_answer:
        return 0.0

    # Define a structured format for evaluation with more detailed guidance
    system_prompt = """You are an expert answer evaluation assistant specializing in nuanced correctness assessment.

    SCORING GUIDELINES:
    - For COMPLETELY CORRECT answers: is_correct=true, confidence between 0.8-1.0
    - For MOSTLY CORRECT answers with minor issues: is_correct=true, confidence between 0.6-0.8
    - For PARTIALLY CORRECT answers with significant gaps: is_correct=false, confidence between 0.3-0.6
    - For MOSTLY INCORRECT answers with some valid elements: is_correct=false, confidence between 0.1-0.3
    - For COMPLETELY INCORRECT answers: is_correct=false, confidence between 0.0-0.1

    Use the FULL RANGE of confidence scores within each category. A score of 0.0 should be reserved only for answers that are completely unrelated or nonsensical.

    Return a JSON with these fields:
    - "is_correct": A boolean value (true/false) indicating if the answer is correct overall
    - "confidence": A precise float between 0.0 and 1.0 following the guidelines above
    - "correctness_category": One of ["COMPLETELY CORRECT", "MOSTLY CORRECT", "PARTIALLY CORRECT", "MOSTLY INCORRECT", "COMPLETELY INCORRECT"]
    - "explanation": Your detailed reasoning for this assessment, including what was right and wrong
    """

    user_prompt = f"""
    Question: {question}

    Candidate Answer: {extracted_answer}

    Reference Answer: {correct_answer}

    Answer Type: {answer_type}

    EVALUATION TASK:
    1. Analyze both the candidate answer and reference answer in relation to the question
    2. For multiple choice questions, check if the selected option matches, even if explanations differ
    3. For exact match questions, evaluate semantic equivalence and conceptual accuracy
    4. Identify any partial correctness or elements of truth in incorrect answers
    5. Apply the scoring guidelines to determine the appropriate confidence score
    6. Classify the answer into one of the correctness categories
    7. Provide a detailed explanation justifying your assessment

    BE DELIBERATE: Ensure your confidence score precisely reflects the degree of correctness - avoid defaulting to standard values.
    """

    try:
        response = openai_client.chat.completions.create(
            model=openai_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.2,  # Slight temperature to allow variation
        )

        result = json.loads(response.choices[0].message.content)
        is_correct = result.get("is_correct", False)
        confidence = result.get("confidence", 0.0)
        category = result.get("correctness_category", "UNKNOWN")

        # Log the detailed assessment for debugging
        print(f"Answer evaluation: {category}, Confidence: {confidence:.2f}")
        print(f"Explanation: {result.get('explanation', 'No explanation provided')[:100]}...")

        # Calculate final score based on correctness category and confidence
        if is_correct:
            return confidence
        else:
            # Scale partial credit based on category
            if category == "PARTIALLY CORRECT":
                return confidence * 0.5  # Higher scaling for partially correct
            elif category == "MOSTLY INCORRECT":
                return confidence * 0.3  # Some credit for mostly incorrect
            else:
                return confidence * 0.1  # Minimal credit for completely incorrect

    except Exception as e:
        print(f"Error evaluating answer correctness: {e}")
        # Fallback to the simpler approach if GPT call fails
        if answer_type == "multipleChoice" and extracted_answer.upper().strip() == correct_answer.upper().strip():
            return 1.0
        else:
            return compute_embedding_similarity(extracted_answer, correct_answer) * 0.7


def compute_reward(question, generated_output, correct_answer, answer_type):
    """
    Compute reward based on the comprehensive formula:
    R = λ1 * C_logic + λ2 * S_step - λ3 * H_halluc - λ4 * W_wrong
    """
    # Extract components
    thinking_text = generated_output.get("thinking", "")
    verification_text = generated_output.get("verification", "")
    conclusion_text = generated_output.get("conclusion", "").strip()

    # Define weights (these can be tuned)
    lambda_logic = 0.3  # Weight for logical consistency
    lambda_step = 0.3  # Weight for stepwise correctness
    lambda_halluc = 0.1  # Weight for hallucination penalty
    lambda_wrong = 0.3  # Weight for wrong answer penalty

    # 1. Evaluate logical consistency (C_logic)
    thinking_steps = extract_reasoning_steps(thinking_text)
    reasoning_graph = build_reasoning_graph(thinking_steps)
    consistency_score = evaluate_logical_consistency(reasoning_graph)

    # 2. Evaluate stepwise correctness (S_step)
    stepwise_score = evaluate_stepwise_correctness(thinking_steps, verification_text)

    # 3. Detect hallucinations (H_halluc)
    hallucination_score = detect_hallucinations(
        thinking_text + " " + verification_text, question
    )

    # 4. Evaluate answer correctness (W_wrong)
    answer_score = evaluate_answer_correctness(conclusion_text, correct_answer, answer_type, question)

    # Calculate final reward using the formula
    reward = (
        lambda_logic * consistency_score
        + lambda_step * stepwise_score
        - lambda_halluc * hallucination_score
        + lambda_wrong * answer_score
    )

    # Print debug information
    print(f"\nQuestion: {question}")
    print(f"Generated Output: {generated_output}")
    print(f"Component Scores:")
    print(f"  - Logical Consistency: {consistency_score:.4f}")
    print(f"  - Stepwise Correctness: {stepwise_score:.4f}")
    print(f"  - Hallucination Penalty: {hallucination_score:.4f}")
    print(f"  - Answer Correctness: {answer_score:.4f}")
    print(f"Reward: {reward:.4f}")

    # Clamp reward to reasonable range
    return max(min(reward, 1.5), -1.0)


def evaluate_model(model, tokenizer, eval_dataset):
    """Evaluate model performance on a dataset."""
    model.eval()
    rewards = []
    correctness_scores = []
    processed_questions = 0
    skipped_questions = 0
    total_questions = len(eval_dataset)

    for i, item in enumerate(eval_dataset):
        try:
            print(f"Processing question {i+1}/{total_questions}")
            question = item["question"]
            answer = item["answer"]
            answer_type = item["answer_type"]

            # Generate text with error handling
            generated_output = generate_text(model, tokenizer, question, answer_type)

            # Skip if generation failed completely
            if generated_output is None:
                print(f"Skipping question {i+1} due to generation failure")
                skipped_questions += 1
                continue

            # Get answer correctness score without fallbacks
            extracted_answer = generated_output.get("conclusion", "").strip()
            answer_correctness_score = evaluate_answer_correctness(
                extracted_answer, answer, answer_type, question
            )

            print(f"Extracted answer: {extracted_answer}")

            # Calculate reward without fallbacks
            reward = compute_reward(question, generated_output, answer, answer_type)

            # Record metrics
            rewards.append(reward)
            correctness_scores.append(answer_correctness_score)
            processed_questions += 1

        except Exception as e:
            print(f"Error processing question {i+1}: {str(e)[:100]}...")
            skipped_questions += 1
            continue

    # Calculate averages
    avg_reward = sum(rewards) / len(rewards) if rewards else 0
    avg_accuracy = sum(correctness_scores) / len(correctness_scores) if correctness_scores else 0

    # Also report the number of answers above threshold for reference
    threshold = 0.7  # You can adjust this
    correct_answers = sum(1 for score in correctness_scores if score >= threshold)

    print(f"Average reward: {avg_reward:.4f}")
    print(f"Average accuracy: {avg_accuracy:.4f}")
    print(f"Binary accuracy @ {threshold}: {correct_answers}/{processed_questions} ({correct_answers/processed_questions:.4f} if processed)")
    print(f"Processed {processed_questions}/{total_questions} questions (skipped {skipped_questions})")

    return avg_reward, avg_accuracy

# PPO training implementation
def train_with_ppo(model, tokenizer, train_dataset, num_epochs=3, batch_size=8, learning_rate=5e-5):
    """Train model using Proximal Policy Optimization (PPO)."""
    print("\n--- Starting PPO Training ---")

    # Create a reference model for KL divergence calculation
    reference_model = copy.deepcopy(model)
    reference_model.eval()  # Set to evaluation mode

    # Prepare optimizer for policy model
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

    # PPO hyperparameters
    ppo_epsilon = 0.2  # Clip parameter for PPO
    value_coef = 0.5  # Value loss coefficient
    kl_coef = 0.1  # KL penalty coefficient

    # Training loop
    total_samples = 0
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")

        # Get indices for this epoch
        indices = list(range(len(train_dataset)))
        random.shuffle(indices)

        # Process mini-batches
        for batch_start in range(0, len(indices), batch_size):
            batch_indices = indices[batch_start:min(batch_start + batch_size, len(indices))]
            batch_rewards = []
            batch_logprobs = []
            batch_ref_logprobs = []
            batch_values = []
            batch_masks = []

            # First pass - collect experiences
            for idx in batch_indices:
                item = train_dataset[idx]
                question = item["question"]
                answer = item["answer"]
                answer_type = item["answer_type"]

                try:
                    # Prepare input
                    prompt = prepare_prompt(question)
                    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

                    # Generate sequence with the current policy
                    with torch.no_grad():
                        outputs = model.generate(
                            input_ids,
                            max_length=512,
                            do_sample=True,
                            temperature=0.7,
                            return_dict_in_generate=True,
                            output_scores=True,
                        )

                    # Get generated tokens and convert to text
                    generated_ids = outputs.sequences[0]
                    generated_output = tokenizer.decode(generated_ids[input_ids.shape[1]:], skip_special_tokens=True)

                    # Compute reward
                    reward_value = float(compute_reward(question, generated_output, answer, answer_type))

                    # Collect action log probabilities for both current and reference models
                    response_ids = generated_ids[input_ids.shape[1]:]

                    # For each token, compute log probabilities
                    logprobs = []
                    ref_logprobs = []
                    values = []

                    for i in range(len(response_ids) - 1):
                        # Calculate for current model
                        context_ids = generated_ids[:input_ids.shape[1] + i]
                        next_token_id = response_ids[i].unsqueeze(0)

                        # Forward pass through current model
                        model_outputs = model(context_ids.unsqueeze(0))
                        logits = model_outputs.logits[:, -1, :]

                        # Calculate log prob of the chosen token
                        probs = F.softmax(logits, dim=-1)
                        log_prob = torch.log(probs[0, next_token_id])
                        logprobs.append(log_prob.item())

                        # Get value prediction if you have a value head
                        # For simplicity, we're using a simple approach here
                        values.append(torch.mean(logits).item())  # Placeholder

                        # Forward pass through reference model
                        with torch.no_grad():
                            ref_outputs = reference_model(context_ids.unsqueeze(0))
                            ref_logits = ref_outputs.logits[:, -1, :]
                            ref_probs = F.softmax(ref_logits, dim=-1)
                            ref_log_prob = torch.log(ref_probs[0, next_token_id])
                            ref_logprobs.append(ref_log_prob.item())

                    # Store batch data
                    sequence_length = len(logprobs)
                    batch_rewards.append([reward_value] * sequence_length)  # Same reward for all tokens
                    batch_logprobs.append(logprobs)
                    batch_ref_logprobs.append(ref_logprobs)
                    batch_values.append(values)
                    batch_masks.append([1.0] * sequence_length)  # Active tokens mask

                except Exception as e:
                    print(f"Error processing sample: {e}")
                    continue

            # If no valid sequences were generated in the batch, skip optimization
            if not batch_rewards:
                continue

            # Compute advantages and returns
            # For simplicity, we're using the reward as advantage directly
            all_logprobs = torch.tensor([item for sublist in batch_logprobs for item in sublist])
            all_ref_logprobs = torch.tensor([item for sublist in batch_ref_logprobs for item in sublist])
            all_rewards = torch.tensor([item for sublist in batch_rewards for item in sublist])
            all_values = torch.tensor([item for sublist in batch_values for item in sublist])
            all_masks = torch.tensor([item for sublist in batch_masks for item in sublist])

            # Calculate ratio and clipped ratio
            ratio = torch.exp(all_logprobs - all_ref_logprobs)
            clipped_ratio = torch.clamp(ratio, 1 - ppo_epsilon, 1 + ppo_epsilon)

            # Calculate losses
            policy_loss = -torch.min(ratio * all_rewards, clipped_ratio * all_rewards).mean()

            # Value loss (if you have a value head)
            value_loss = F.mse_loss(all_values, all_rewards)

            # KL divergence penalty
            kl_div = (all_ref_logprobs - all_logprobs).mean()

            # Total loss
            loss = policy_loss + value_coef * value_loss + kl_coef * kl_div

            # Optimize
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Print statistics
            avg_reward = all_rewards.mean().item()
            print(f"Batch {batch_start//batch_size + 1}: Avg reward: {avg_reward:.4f}, Loss: {loss.item():.6f}")
            total_samples += len(batch_indices)

        # Update reference model at the end of each epoch
        reference_model = copy.deepcopy(model)
        reference_model.eval()

    print("--- PPO Training Complete ---")
    return model

# Create proper train, validation, and test splits
indices = list(range(len(hle_dataset)))
random.shuffle(indices)

train_size = int(0.7 * len(indices))
val_size = int(0.15 * len(indices))
test_size = len(indices) - train_size - val_size

train_indices = indices[:train_size]
val_indices = indices[train_size:train_size+val_size]
test_indices = indices[train_size+val_size:]

# Use the select method to maintain dataset format
train_dataset = hle_dataset.select(train_indices)
val_dataset = hle_dataset.select(val_indices)
test_dataset = hle_dataset.select(test_indices)

# PPO training (using TRAIN set only)
fine_tuned_model = train_with_ppo(
    model,
    tokenizer,
    train_dataset,  # Use the full train dataset
    num_epochs=3,
    batch_size=10,
    learning_rate=1e-5
)

from google.colab import drive
drive.mount('/content/drive')

drive_output_dir = "/content/drive/MyDrive/neuro_240/fine_tuned_model"
fine_tuned_model.save_pretrained(drive_output_dir)
tokenizer.save_pretrained(drive_output_dir)

print(f"Model saved to Google Drive at {drive_output_dir}")

# Initial evaluation (baseline) on TEST set
print("\n--- Evaluating baseline model ---")

test_subset = test_dataset.select(range(min(1, len(test_dataset))))
baseline_reward, baseline_accuracy = evaluate_model(model, tokenizer, test_subset)

# Final evaluation after fine-tuning (on the SAME test subset)
print("\n--- Evaluating fine-tuned model ---")
final_reward, final_accuracy = evaluate_model(fine_tuned_model, tokenizer, test_subset)

print(f"Reward Improvement: {final_reward - baseline_reward:.4f}")
print(f"Accuracy Improvement: {final_accuracy - baseline_accuracy:.4f} ({(final_accuracy - baseline_accuracy) * 100:.2f}%)")

print("\n--- Further Evaluation on Validation Set ---")
val_subset = test_dataset.select(range(min(100, len(val_dataset))))
full_test_reward, full_test_accuracy = evaluate_model(fine_tuned_model, tokenizer, val_dataset)
print(f"Full Test Set - Reward: {full_test_reward:.4f}, Accuracy: {full_test_accuracy:.4f}")

"""## Model Experiments (Fine Tuning + Evaluation)"""

from google.colab import drive
drive.mount('/content/drive')

import os
drive_models_dir = "/content/drive/MyDrive/fine_tuned_models"
os.makedirs(drive_models_dir, exist_ok=True)

training_results = {}

# Train and save each model
for model_key, model_path in LIGHTWEIGHT_MODELS.items():
    print(f"\n{'='*50}")
    print(f"Training {model_key} ({model_path})")
    print(f"{'='*50}")

    try:
        # Setup the model
        tokenizer, model = setup_model(model_path)

        # Create model-specific output directory
        model_output_dir = f"./fine_tuned_models/{model_key}"
        os.makedirs(model_output_dir, exist_ok=True)

        # PPO training
        fine_tuned_model = train_with_ppo(
            model,
            tokenizer,
            train_dataset,
            num_epochs=3,
            batch_size=10,
            learning_rate=1e-5
        )

        # Save the fine-tuned model
        fine_tuned_model.save_pretrained(model_output_dir)
        tokenizer.save_pretrained(model_output_dir)

        training_results[model_key] = "Success"

        # Clear memory
        del model
        del fine_tuned_model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error training {model_key}: {str(e)}")
        training_results[model_key] = f"Failed: {str(e)[:100]}..."
        continue

for model_key, result in training_results.items():
    print(f"{model_key}: {result}")
print("All models processed. Fine-tuned models are saved in ./fine_tuned_models/")

def run_model_evaluations(test_dataset, fine_tuned_base_dir="/content/drive/MyDrive/fine_tuned_models"):

    """
    Run evaluations on all models before and after fine-tuning and return results as a DataFrame.

    Args:
        test_dataset: Dataset to evaluate models on

    Returns:
        pandas.DataFrame: Results of model evaluations
    """
    import pandas as pd

    # Define the column headers
    headers = [
        "Model",
        "LogicalConsistency_Before", "LogicalConsistency_After", "LogicalConsistency_AbsChange", "LogicalConsistency_PctChange",
        "StepwiseCorrectness_Before", "StepwiseCorrectness_After", "StepwiseCorrectness_AbsChange", "StepwiseCorrectness_PctChange",
        "HallucinationPenalty_Before", "HallucinationPenalty_After", "HallucinationPenalty_AbsChange", "HallucinationPenalty_PctChange",
        "AnswerCorrectness_Before", "AnswerCorrectness_After", "AnswerCorrectness_AbsChange", "AnswerCorrectness_PctChange",
        "OverallReward_Before", "OverallReward_After", "OverallReward_AbsChange", "OverallReward_PctChange"
    ]

    # Create list to store results
    results = []

    # Use a small subset of test data for quick evaluation during development
    # Remove this line for full evaluation
    eval_subset = test_dataset.select(range(min(5, len(test_dataset))))

    # Evaluate each model
    for model_key, model_path in LIGHTWEIGHT_MODELS.items():
        print(f"\n{'='*50}")
        print(f"Evaluating {model_key} ({model_path})")
        print(f"{'='*50}")

        # Load the baseline model
        tokenizer, baseline_model = setup_model(model_path)

        # Get the path to fine-tuned model
        fine_tuned_path = f"{fine_tuned_base_dir}/{model_key}"

        # Check if fine-tuned model exists
        if not os.path.exists(fine_tuned_path):
            print(f"Fine-tuned model for {model_key} not found at {fine_tuned_path}. Skipping.")
            continue

        # Initialize component metrics dictionaries
        before_metrics = {
            "logical_consistency": [],
            "stepwise_correctness": [],
            "hallucination_penalty": [],
            "answer_correctness": [],
            "overall_reward": []
        }

        after_metrics = {
            "logical_consistency": [],
            "stepwise_correctness": [],
            "hallucination_penalty": [],
            "answer_correctness": [],
            "overall_reward": []
        }

        # Evaluate baseline model
        print("\n--- Evaluating baseline model ---")
        for i, item in enumerate(eval_subset):
            try:
                print(f"Processing question {i+1}/{len(eval_subset)}")
                question = item["question"]
                answer = item["answer"]
                answer_type = item["answer_type"]

                # Generate output with baseline model
                baseline_output = generate_text(baseline_model, tokenizer, question, answer_type)

                # Skip if generation failed
                if baseline_output is None:
                    print(f"Skipping question {i+1} due to generation failure")
                    continue

                # Extract metrics
                thinking_text = baseline_output.get("thinking", "")
                verification_text = baseline_output.get("verification", "")
                conclusion_text = baseline_output.get("conclusion", "").strip()

                # Calculate individual component scores
                thinking_steps = extract_reasoning_steps(thinking_text)
                reasoning_graph = build_reasoning_graph(thinking_steps)
                consistency_score = evaluate_logical_consistency(reasoning_graph)
                stepwise_score = evaluate_stepwise_correctness(thinking_steps, verification_text)
                hallucination_score = detect_hallucinations(thinking_text + " " + verification_text, question)
                answer_score = evaluate_answer_correctness(conclusion_text, answer, answer_type, question)

                # Calculate reward
                reward = compute_reward(question, baseline_output, answer, answer_type)

                # Store metrics
                before_metrics["logical_consistency"].append(consistency_score)
                before_metrics["stepwise_correctness"].append(stepwise_score)
                before_metrics["hallucination_penalty"].append(hallucination_score)
                before_metrics["answer_correctness"].append(answer_score)
                before_metrics["overall_reward"].append(reward)

            except Exception as e:
                print(f"Error processing question {i+1}: {str(e)[:100]}...")
                continue

        # Load fine-tuned model
        tokenizer, fine_tuned_model = setup_model(fine_tuned_path)

        # Evaluate fine-tuned model
        print("\n--- Evaluating fine-tuned model ---")
        for i, item in enumerate(eval_subset):
            try:
                print(f"Processing question {i+1}/{len(eval_subset)}")
                question = item["question"]
                answer = item["answer"]
                answer_type = item["answer_type"]

                # Generate output with fine-tuned model
                finetuned_output = generate_text(fine_tuned_model, tokenizer, question, answer_type)

                # Skip if generation failed
                if finetuned_output is None:
                    print(f"Skipping question {i+1} due to generation failure")
                    continue

                # Extract metrics
                thinking_text = finetuned_output.get("thinking", "")
                verification_text = finetuned_output.get("verification", "")
                conclusion_text = finetuned_output.get("conclusion", "").strip()

                # Calculate individual component scores
                thinking_steps = extract_reasoning_steps(thinking_text)
                reasoning_graph = build_reasoning_graph(thinking_steps)
                consistency_score = evaluate_logical_consistency(reasoning_graph)
                stepwise_score = evaluate_stepwise_correctness(thinking_steps, verification_text)
                hallucination_score = detect_hallucinations(thinking_text + " " + verification_text, question)
                answer_score = evaluate_answer_correctness(conclusion_text, answer, answer_type, question)

                # Calculate reward
                reward = compute_reward(question, finetuned_output, answer, answer_type)

                # Store metrics
                after_metrics["logical_consistency"].append(consistency_score)
                after_metrics["stepwise_correctness"].append(stepwise_score)
                after_metrics["hallucination_penalty"].append(hallucination_score)
                after_metrics["answer_correctness"].append(answer_score)
                after_metrics["overall_reward"].append(reward)

            except Exception as e:
                print(f"Error processing question {i+1}: {str(e)[:100]}...")
                continue

        # Calculate average metrics
        before_avg = {}
        after_avg = {}
        abs_changes = {}
        pct_changes = {}

        for metric in before_metrics:
            if before_metrics[metric] and after_metrics[metric]:
                before_avg[metric] = sum(before_metrics[metric]) / len(before_metrics[metric])
                after_avg[metric] = sum(after_metrics[metric]) / len(after_metrics[metric])
                abs_changes[metric] = after_avg[metric] - before_avg[metric]

                # Calculate percentage change (avoid division by zero)
                if before_avg[metric] != 0:
                    pct_changes[metric] = (abs_changes[metric] / before_avg[metric]) * 100
                else:
                    pct_changes[metric] = 0

        # Create a row for this model
        row_data = {
            "Model": model_key,

            # Logical Consistency
            "LogicalConsistency_Before": before_avg.get("logical_consistency", 0),
            "LogicalConsistency_After": after_avg.get("logical_consistency", 0),
            "LogicalConsistency_AbsChange": abs_changes.get("logical_consistency", 0),
            "LogicalConsistency_PctChange": pct_changes.get("logical_consistency", 0),

            # Stepwise Correctness
            "StepwiseCorrectness_Before": before_avg.get("stepwise_correctness", 0),
            "StepwiseCorrectness_After": after_avg.get("stepwise_correctness", 0),
            "StepwiseCorrectness_AbsChange": abs_changes.get("stepwise_correctness", 0),
            "StepwiseCorrectness_PctChange": pct_changes.get("stepwise_correctness", 0),

            # Hallucination Penalty
            "HallucinationPenalty_Before": before_avg.get("hallucination_penalty", 0),
            "HallucinationPenalty_After": after_avg.get("hallucination_penalty", 0),
            "HallucinationPenalty_AbsChange": abs_changes.get("hallucination_penalty", 0),
            "HallucinationPenalty_PctChange": pct_changes.get("hallucination_penalty", 0),

            # Answer Correctness
            "AnswerCorrectness_Before": before_avg.get("answer_correctness", 0),
            "AnswerCorrectness_After": after_avg.get("answer_correctness", 0),
            "AnswerCorrectness_AbsChange": abs_changes.get("answer_correctness", 0),
            "AnswerCorrectness_PctChange": pct_changes.get("answer_correctness", 0),

            # Overall Reward
            "OverallReward_Before": before_avg.get("overall_reward", 0),
            "OverallReward_After": after_avg.get("overall_reward", 0),
            "OverallReward_AbsChange": abs_changes.get("overall_reward", 0),
            "OverallReward_PctChange": pct_changes.get("overall_reward", 0),
        }

        # Add row to results
        results.append(row_data)

        print(f"\nEvaluation for {model_key} completed")

        # Clear memory
        del baseline_model
        del fine_tuned_model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # Create DataFrame from results
    results_df = pd.DataFrame(results)

    print(f"\nAll evaluations completed.")
    return results_df

# Run the evaluations on test dataset and get DataFrame
results_df = run_model_evaluations(test_dataset)

# Save DataFrame to CSV
output_csv = "model_performance_results.csv"
results_df.to_csv(output_csv, index=False)
print(f"Results saved to {output_csv}")

print("Results saved to model_performance_results.csv")

"""## Analysis"""

from pandas import read_csv
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from matplotlib.path import Path
from matplotlib.spines import Spine
from matplotlib.projections.polar import PolarAxes
from matplotlib.projections import register_projection
from matplotlib.patches import RegularPolygon, Circle

results_df = read_csv("model_performance_results.csv")

# 1. Overall Reward Comparison (Before vs After)
plt.figure(figsize=(12, 6))
models = results_df["Model"].tolist()
x = np.arange(len(models))
width = 0.35

plt.bar(x - width/2, results_df["OverallReward_Before"], width, label="Before Fine-Tuning")
plt.bar(x + width/2, results_df["OverallReward_After"], width, label="After Fine-Tuning")

plt.xlabel("Models")
plt.ylabel("Overall Reward")
plt.title("Overall Reward Before and After Fine-Tuning")
plt.xticks(x, models, rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

# 2. Percentage Improvement in Overall Reward
plt.figure(figsize=(12, 6))
colors = ['green' if x > 0 else 'red' for x in results_df["OverallReward_PctChange"]]

# Sort by percentage improvement
sorted_df = results_df.sort_values("OverallReward_PctChange", ascending=False)

plt.bar(sorted_df["Model"], sorted_df["OverallReward_PctChange"], color=colors)
plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)

plt.xlabel("Models")
plt.ylabel("% Change in Overall Reward")
plt.title("Percentage Improvement in Overall Reward After Fine-Tuning")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 3. Component-wise Comparison across Models
components = ["LogicalConsistency", "StepwiseCorrectness", "HallucinationPenalty", "AnswerCorrectness"]

for component in components:
    plt.figure(figsize=(12, 6))

    # Sort by improvement in this component
    sorted_df = results_df.sort_values(f"{component}_AbsChange", ascending=False)

    plt.bar(sorted_df["Model"], sorted_df[f"{component}_Before"], width=0.35, label="Before")
    plt.bar(sorted_df["Model"], sorted_df[f"{component}_After"], width=0.35, alpha=0.5, label="After")

    plt.xlabel("Models")
    plt.ylabel(component)
    plt.title(f"{component} Before and After Fine-Tuning")
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()

# 4. Heatmap of percentage changes
plt.figure(figsize=(12, 8))

# Create a dataframe with just the percentage changes
pct_df = results_df[["Model"] + [c for c in results_df.columns if c.endswith("PctChange")]]
pct_df = pct_df.set_index("Model")

# Create readable column names
pct_df.columns = [c.replace("_PctChange", "") for c in pct_df.columns]

# For HallucinationPenalty, lower is better, so invert the sign
pct_df["HallucinationPenalty"] = -pct_df["HallucinationPenalty"]

# Plot heatmap
sns.heatmap(pct_df, annot=True, cmap="RdYlGn", center=0, fmt=".1f")
plt.title("Percentage Changes in Performance Metrics After Fine-Tuning")
plt.tight_layout()
plt.show()

# 5. Correlation analysis between metrics
plt.figure(figsize=(10, 8))

# Extract metrics data
metric_cols = [c for c in results_df.columns if c.endswith("_After") or c.endswith("AbsChange")]
corr_df = results_df[metric_cols].corr()

# Plot correlation heatmap
sns.heatmap(corr_df, annot=True, cmap="coolwarm", vmin=-1, vmax=1, fmt=".2f")
plt.title("Correlation Between Metrics")
plt.tight_layout()
plt.show()

# 7. Composite bar chart of all component changes
plt.figure(figsize=(14, 8))

# Prepare data
components = ["LogicalConsistency", "StepwiseCorrectness", "AnswerCorrectness"]
models = results_df["Model"].tolist()

bar_width = 0.25
r1 = np.arange(len(models))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]

# Get absolute changes for each component
lc_changes = results_df["LogicalConsistency_AbsChange"].tolist()
sc_changes = results_df["StepwiseCorrectness_AbsChange"].tolist()
ac_changes = results_df["AnswerCorrectness_AbsChange"].tolist()

plt.bar(r1, lc_changes, width=bar_width, label='Logical Consistency')
plt.bar(r2, sc_changes, width=bar_width, label='Stepwise Correctness')
plt.bar(r3, ac_changes, width=bar_width, label='Answer Correctness')

plt.xlabel('Model')
plt.ylabel('Absolute Improvement')
plt.title('Component-wise Improvement by Model')
plt.xticks([r + bar_width for r in range(len(models))], models, rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

# 8. Textual Summary of Key Findings
# Overall best model
best_model = results_df.loc[results_df["OverallReward_AbsChange"].idxmax()]["Model"]
best_pct = results_df.loc[results_df["OverallReward_PctChange"].idxmax()]["Model"]

print("## Overall Performance")
print(f"- Best absolute improvement: {best_model}")
print(f"- Best percentage improvement: {best_pct}\n")

# Component-wise best models
print("## Component-wise Best Performers")

for component in ["LogicalConsistency", "StepwiseCorrectness", "HallucinationPenalty", "AnswerCorrectness"]:
    if component == "HallucinationPenalty":
        # For hallucination, lower is better, so we want the most negative change
        best = results_df.loc[results_df[f"{component}_AbsChange"].idxmin()]["Model"]
        print(f"- Best {component} reduction: {best}")
    else:
        best = results_df.loc[results_df[f"{component}_AbsChange"].idxmax()]["Model"]
        print(f"- Best {component} improvement: {best}")

print("\n## Model Rankings by Overall Improvement")
sorted_models = results_df.sort_values("OverallReward_AbsChange", ascending=False)["Model"].tolist()
for i, model in enumerate(sorted_models):
    print(f"{i+1}. {model}")

# Identify trends and patterns
print("\n## Key Findings and Trends")

# Check if any component consistently improved across models
components = ["LogicalConsistency", "StepwiseCorrectness", "HallucinationPenalty", "AnswerCorrectness"]
for component in components:
    changes = results_df[f"{component}_AbsChange"].tolist()
    if component == "HallucinationPenalty":
        if all(x < 0 for x in changes):
            print(f"- All models showed a reduction in {component}")
        elif sum(x < 0 for x in changes) >= len(changes)/2:
            print(f"- Majority of models showed a reduction in {component}")
    else:
        if all(x > 0 for x in changes):
            print(f"- All models showed improvement in {component}")
        elif sum(x > 0 for x in changes) >= len(changes)/2:
            print(f"- Majority of models showed improvement in {component}")

# Note any unexpected findings
print("\n## Unexpected Findings")
# Check for cases where hallucination increased
hall_increased = results_df[results_df["HallucinationPenalty_AbsChange"] > 0]["Model"].tolist()
if hall_increased:
    print(f"- The following models showed increased hallucination after fine-tuning: {', '.join(hall_increased)}")

# Check for components that decreased
for component in ["LogicalConsistency", "StepwiseCorrectness", "AnswerCorrectness"]:
    decreased = results_df[results_df[f"{component}_AbsChange"] < 0]["Model"].tolist()
    if decreased:
        print(f"- {component} decreased in these models: {', '.join(decreased)}")

# Calculate average change across all models for each metric
def analyze_average_changes(results_df):
    # Define the metrics we want to analyze
    metrics = [
        "LogicalConsistency",
        "StepwiseCorrectness",
        "HallucinationPenalty",
        "AnswerCorrectness",
        "OverallReward"
    ]

    # Dictionary to store results
    avg_changes = {}

    # Calculate averages for each metric
    for metric in metrics:
        # Calculate average absolute change
        abs_change_col = f"{metric}_AbsChange"
        avg_abs_change = results_df[abs_change_col].mean()

        # Calculate average percentage change
        pct_change_col = f"{metric}_PctChange"
        avg_pct_change = results_df[pct_change_col].mean()

        # Store results
        avg_changes[metric] = {
            "avg_abs_change": avg_abs_change,
            "avg_pct_change": avg_pct_change
        }

    # Print the results in a readable format
    print("Average Changes Across All Models:")
    print("-" * 50)
    print(f"{'Metric':<25} {'Abs Change':<15} {'% Change':<15}")
    print("-" * 50)

    for metric, changes in avg_changes.items():
        abs_change = changes["avg_abs_change"]
        pct_change = changes["avg_pct_change"]

        # Format special for Hallucination Penalty (where negative is good)
        if metric == "HallucinationPenalty":
            direction = "↓" if abs_change < 0 else "↑"
        else:
            direction = "↑" if abs_change > 0 else "↓"

        print(f"{metric:<25} {abs_change:+.6f} {direction:<5} {pct_change:+.2f}%")

    # Return the dictionary for further analysis if needed
    return avg_changes

# Find which models performed best for each metric
def find_best_performers(results_df):
    metrics = ["LogicalConsistency", "StepwiseCorrectness", "HallucinationPenalty", "AnswerCorrectness"]

    print("\nBest Performing Models by Metric:")
    print("-" * 50)

    for metric in metrics:
        abs_change_col = f"{metric}_AbsChange"

        if metric == "HallucinationPenalty":
            # For hallucination, lower is better, so we want the most negative change
            best_idx = results_df[abs_change_col].idxmin()
            best_change = results_df.loc[best_idx, abs_change_col]
            direction = "reduction"
        else:
            # For other metrics, higher is better
            best_idx = results_df[abs_change_col].idxmax()
            best_change = results_df.loc[best_idx, abs_change_col]
            direction = "improvement"

        best_model = results_df.loc[best_idx, "Model"]

        print(f"Best {metric} {direction}: {best_model} ({best_change:+.6f})")

    # Also find best overall reward improvement
    best_overall_idx = results_df["OverallReward_AbsChange"].idxmax()
    best_overall_model = results_df.loc[best_overall_idx, "Model"]
    best_overall_change = results_df.loc[best_overall_idx, "OverallReward_AbsChange"]

    print(f"Best Overall Reward improvement: {best_overall_model} ({best_overall_change:+.6f})")

# Execute both analysis functions
avg_changes = analyze_average_changes(results_df)
find_best_performers(results_df)

# Optional: Calculate correlation between model performance metrics
print("\nCorrelation Between Metric Changes:")
change_cols = [col for col in results_df.columns if col.endswith("AbsChange")]
correlation = results_df[change_cols].corr()
print(correlation)

# Add model parameter sizes
def analyze_model_size_relationship(results_df):
    # Add parameter count for each model (in millions)
    model_sizes = {
        "GPT-2": 124,  # 124M parameters
        "TinyLlama-1.1B": 1100,  # 1.1B parameters
        "OPT-1.3B": 1300,  # 1.3B parameters
        "Pythia-1.4B": 1400,  # 1.4B parameters
        "Flan-T5-Small": 80,  # 80M parameters
        "Phi-2": 2700,  # 2.7B parameters
        "StableLM-3B": 3000,  # 3B parameters
    }

    # Create a copy of results_df with model size information
    analysis_df = results_df.copy()
    analysis_df['ModelSizeM'] = analysis_df['Model'].map(model_sizes)

    # Sort by model size for easier visualization
    analysis_df = analysis_df.sort_values('ModelSizeM')

    # Print the sorted data to see patterns
    print("Models sorted by size:")
    print("-" * 60)
    print(f"{'Model':<15} {'Size (M)':<10} {'Overall Reward Improvement':<25} {'% Improvement':<15}")
    print("-" * 60)

    for _, row in analysis_df.iterrows():
        print(f"{row['Model']:<15} {row['ModelSizeM']:<10} {row['OverallReward_AbsChange']:+.6f}{' ':16} {row['OverallReward_PctChange']:+.2f}%")

    # Calculate correlation between model size and improvements
    print("\nCorrelations with Model Size:")
    print("-" * 40)

    metrics = ["LogicalConsistency", "StepwiseCorrectness", "HallucinationPenalty", "AnswerCorrectness", "OverallReward"]

    for metric in metrics:
        abs_corr = analysis_df['ModelSizeM'].corr(analysis_df[f"{metric}_AbsChange"])
        pct_corr = analysis_df['ModelSizeM'].corr(analysis_df[f"{metric}_PctChange"])
        print(f"{metric:<20} Abs Change: {abs_corr:+.4f}   % Change: {pct_corr:+.4f}")

    # Test for quadratic relationship (inverted U-shape)
    import numpy as np
    from scipy import stats
    import matplotlib.pyplot as plt

    print("\nTesting for quadratic relationship:")
    print("-" * 40)

    # Function to test quadratic relationship and return R-squared
    def test_quadratic(x, y):
        # Fit quadratic model: y = ax² + bx + c
        coeffs = np.polyfit(x, y, 2)
        # Create polynomial function
        p = np.poly1d(coeffs)
        # Calculate predicted values
        y_pred = p(x)
        # Calculate R-squared
        ss_total = np.sum((y - np.mean(y))**2)
        ss_residual = np.sum((y - y_pred)**2)
        r_squared = 1 - (ss_residual / ss_total)

        # Check if it's an inverted U shape (negative quadratic term)
        is_inverted_u = coeffs[0] < 0

        return r_squared, is_inverted_u, coeffs

    # Test quadratic relationship for overall reward
    x = analysis_df['ModelSizeM'].values
    y = analysis_df['OverallReward_AbsChange'].values

    r_squared, is_inverted_u, coeffs = test_quadratic(x, y)

    print(f"Overall Reward Improvement:")
    print(f"R-squared for quadratic fit: {r_squared:.4f}")
    print(f"Is inverted U-shape: {is_inverted_u}")
    print(f"Quadratic coefficients: a={coeffs[0]:.8f}, b={coeffs[1]:.6f}, c={coeffs[2]:.6f}")

    # Calculate the size at which improvement peaks (if it's an inverted U)
    if is_inverted_u:
        peak_size = -coeffs[1] / (2 * coeffs[0])
        print(f"Peak improvement occurs at approximately {peak_size:.1f}M parameters")

    # Generate data for plotting the quadratic curve if needed
    return analysis_df

# Run the analysis
analysis_df = analyze_model_size_relationship(results_df)


import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(10, 6))

# Scatter plot of model size vs improvement
plt.scatter(analysis_df['ModelSizeM'], analysis_df['OverallReward_AbsChange'],
            s=100, alpha=0.7, label='Models')

# Add model names as labels
for i, row in analysis_df.iterrows():
    plt.annotate(row['Model'],
                (row['ModelSizeM'], row['OverallReward_AbsChange']),
                xytext=(5, 5), textcoords='offset points')

# Generate points for quadratic fit curve
x = np.linspace(0, max(analysis_df['ModelSizeM']) * 1.1, 100)
coeffs = np.polyfit(analysis_df['ModelSizeM'], analysis_df['OverallReward_AbsChange'], 2)
y = coeffs[0] * x**2 + coeffs[1] * x + coeffs[2]
plt.plot(x, y, 'r--', label='Quadratic Fit')

plt.xlabel('Model Size (Million Parameters)')
plt.ylabel('Overall Reward Improvement')
plt.title('Relationship Between Model Size and Improvement')
plt.grid(True, alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

# 5. Radar charts for models
from matplotlib.path import Path
from matplotlib.spines import Spine
from matplotlib.projections.polar import PolarAxes
from matplotlib.projections import register_projection
from matplotlib.patches import RegularPolygon, Circle

def radar_factory(num_vars, frame='circle'):
    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)

    class RadarAxes(PolarAxes):
        name = 'radar'

        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.set_theta_zero_location('N')
            self.varlabels = None

        def fill(self, *args, **kwargs):
            return super().fill(closed=True, *args, **kwargs)

        def plot(self, *args, **kwargs):
            lines = super().plot(*args, **kwargs)
            for line in lines:
                self._close_line(line)
            return lines

        def _close_line(self, line):
            x, y = line.get_data()
            if x[0] != x[-1]:
                x = np.concatenate((x, [x[0]]))
                y = np.concatenate((y, [y[0]]))
                line.set_data(x, y)

        def set_varlabels(self, labels):
            self.set_thetagrids(np.degrees(theta), labels)
            self.varlabels = labels

        def _gen_axes_patch(self):
            if frame == 'circle':
                return Circle((0.5, 0.5), 0.5)
            elif frame == 'polygon':
                return RegularPolygon((0.5, 0.5), num_vars, radius=0.5, edgecolor="k")
            else:
                raise ValueError("unknown value for 'frame': %s" % frame)

        def draw(self, renderer):
            if frame == 'circle':
                patch = Circle((0.5, 0.5), 0.5)
                patch.set_transform(self.transAxes)
                patch.set_clip_on(False)
                patch.set_alpha(0.1)
                self.add_patch(patch)
            elif frame == 'polygon':
                patch = RegularPolygon((0.5, 0.5), num_vars, radius=0.5)
                patch.set_transform(self.transAxes)
                patch.set_clip_on(False)
                patch.set_alpha(0.1)
                self.add_patch(patch)

            for i in range(num_vars):
                t1 = (i * 2*np.pi / num_vars) % (2*np.pi)
                t2 = (i + 0.5) * 2*np.pi / num_vars
                self.plot([t1, t1], [0, 1], '-', lw=0.8, color='gray', alpha=0.3)
                if self.varlabels is not None:
                    self.text(t2, 1.05, self.varlabels[i],
                           horizontalalignment='center',
                           verticalalignment='center',
                           size=10)

            super().draw(renderer)

    register_projection(RadarAxes)
    return theta

# Create radar charts for each model
metric_labels = ["Logical\nConsistency", "Stepwise\nCorrectness", "Hallucination\nPenalty (inv)", "Answer\nCorrectness"]
N = len(metric_labels)
theta = radar_factory(N, frame='polygon')

# Show at most 3 radar charts per row
models_per_row = 3
num_rows = (len(results_df) + models_per_row - 1) // models_per_row
fig, axes = plt.subplots(num_rows, models_per_row, figsize=(15, 5*num_rows),
                         subplot_kw=dict(projection='radar'))
axes = np.array(axes).flatten()

for i, (_, row) in enumerate(results_df.iterrows()):
    if i >= len(axes):
        break

    ax = axes[i]
    model_name = row["Model"]

    # Get the metrics, inverting hallucination penalty (lower is better)
    before_data = [
        row["LogicalConsistency_Before"],
        row["StepwiseCorrectness_Before"],
        1 - row["HallucinationPenalty_Before"],  # Invert so higher is better
        row["AnswerCorrectness_Before"]
    ]

    after_data = [
        row["LogicalConsistency_After"],
        row["StepwiseCorrectness_After"],
        1 - row["HallucinationPenalty_After"],  # Invert so higher is better
        row["AnswerCorrectness_After"]
    ]

    # Plot the data
    ax.plot(theta, before_data, 'o-', linewidth=2, label='Before')
    ax.fill(theta, before_data, alpha=0.25)
    ax.plot(theta, after_data, 'o-', linewidth=2, label='After')
    ax.fill(theta, after_data, alpha=0.25)

    ax.set_varlabels(metric_labels)
    ax.set_title(f"{model_name}", size=12)
    ax.legend(loc='upper right', fontsize=8)

# Hide any unused subplots
for j in range(i+1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.show()

def analyze_results(results_df):
    """
    Create plots and analyze model performance results.

    Args:
        results_df: DataFrame containing the model evaluation results
    """
    print("Analyzing model performance results...")

    # Set up plotting style
    plt.style.use('ggplot')
    sns.set_palette("Set2")

    # Create directory for plots
    import os
    plots_dir = "./performance_plots"
    os.makedirs(plots_dir, exist_ok=True)

    # Make sure results_df has the right column types
    for col in results_df.columns:
        if col != "Model" and not col.endswith("PctChange"):
            results_df[col] = pd.to_numeric(results_df[col])

    # 1. Overall Reward Comparison (Before vs After)
    plt.figure(figsize=(12, 6))
    models = results_df["Model"].tolist()
    x = np.arange(len(models))
    width = 0.35

    plt.bar(x - width/2, results_df["OverallReward_Before"], width, label="Before Fine-Tuning")
    plt.bar(x + width/2, results_df["OverallReward_After"], width, label="After Fine-Tuning")

    plt.xlabel("Models")
    plt.ylabel("Overall Reward")
    plt.title("Overall Reward Before and After Fine-Tuning")
    plt.xticks(x, models, rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{plots_dir}/overall_reward_comparison.png", dpi=300)
    plt.close()

    # 2. Percentage Improvement in Overall Reward
    plt.figure(figsize=(12, 6))
    colors = ['green' if x > 0 else 'red' for x in results_df["OverallReward_PctChange"]]

    # Sort by percentage improvement
    sorted_df = results_df.sort_values("OverallReward_PctChange", ascending=False)

    plt.bar(sorted_df["Model"], sorted_df["OverallReward_PctChange"], color=colors)
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)

    plt.xlabel("Models")
    plt.ylabel("% Change in Overall Reward")
    plt.title("Percentage Improvement in Overall Reward After Fine-Tuning")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f"{plots_dir}/overall_reward_pct_improvement.png", dpi=300)
    plt.close()

    # 3. Component-wise Comparison across Models
    components = ["LogicalConsistency", "StepwiseCorrectness", "HallucinationPenalty", "AnswerCorrectness"]

    for component in components:
        plt.figure(figsize=(12, 6))

        # Sort by improvement in this component
        sorted_df = results_df.sort_values(f"{component}_AbsChange", ascending=False)

        plt.bar(sorted_df["Model"], sorted_df[f"{component}_Before"], width, label="Before")
        plt.bar(sorted_df["Model"], sorted_df[f"{component}_After"], width, alpha=0.5, label="After")

        plt.xlabel("Models")
        plt.ylabel(component)
        plt.title(f"{component} Before and After Fine-Tuning")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/{component}_comparison.png", dpi=300)
        plt.close()

    # 4. Heatmap of percentage changes
    plt.figure(figsize=(12, 8))

    # Create a dataframe with just the percentage changes
    pct_df = results_df[["Model"] + [c for c in results_df.columns if c.endswith("PctChange")]]
    pct_df = pct_df.set_index("Model")

    # Create readable column names
    pct_df.columns = [c.replace("_PctChange", "") for c in pct_df.columns]

    # For HallucinationPenalty, lower is better, so invert the sign
    pct_df["HallucinationPenalty"] = -pct_df["HallucinationPenalty"]

    # Plot heatmap
    sns.heatmap(pct_df, annot=True, cmap="RdYlGn", center=0, fmt=".1f")
    plt.title("Percentage Changes in Performance Metrics After Fine-Tuning")
    plt.tight_layout()
    plt.savefig(f"{plots_dir}/pct_change_heatmap.png", dpi=300)
    plt.close()

    # 5. Spider/Radar chart for each model showing before/after across all metrics
    from matplotlib.path import Path
    from matplotlib.spines import Spine
    from matplotlib.projections.polar import PolarAxes
    from matplotlib.projections import register_projection

    def radar_factory(num_vars, frame='circle'):
        theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)
        class RadarAxes(PolarAxes):
            name = 'radar'

            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.set_theta_zero_location('N')
                self.varlabels = None  # Add this line to store labels

            def fill(self, *args, **kwargs):
                return super().fill(closed=True, *args, **kwargs)

            def plot(self, *args, **kwargs):
                lines = super().plot(*args, **kwargs)
                for line in lines:
                    self._close_line(line)
                return lines

            def _close_line(self, line):
                x, y = line.get_data()
                if x[0] != x[-1]:
                    x = np.concatenate((x, [x[0]]))
                    y = np.concatenate((y, [y[0]]))
                    line.set_data(x, y)

            def set_varlabels(self, labels):
                self.set_thetagrids(np.degrees(theta), labels)
                self.varlabels = labels  # Store the labels

            def _gen_axes_patch(self):
                if frame == 'circle':
                    return Circle((0.5, 0.5), 0.5)
                elif frame == 'polygon':
                    return RegularPolygon((0.5, 0.5), num_vars, radius=0.5, edgecolor="k")
                else:
                    raise ValueError("unknown value for 'frame': %s" % frame)

            def draw(self, renderer):
                if frame == 'circle':
                    patch = Circle((0.5, 0.5), 0.5)
                    patch.set_transform(self.transAxes)
                    patch.set_clip_on(False)
                    patch.set_alpha(0.1)
                    self.add_patch(patch)
                elif frame == 'polygon':
                    patch = RegularPolygon((0.5, 0.5), num_vars, radius=0.5)
                    patch.set_transform(self.transAxes)
                    patch.set_clip_on(False)
                    patch.set_alpha(0.1)
                    self.add_patch(patch)

                for i in range(num_vars):
                    t1 = (i * 2*np.pi / num_vars) % (2*np.pi)
                    t2 = (i + 0.5) * 2*np.pi / num_vars
                    self.plot([t1, t1], [0, 1], '-', lw=0.8, color='gray', alpha=0.3)
                    # Only add labels if they are set
                    if self.varlabels is not None:  # Use stored labels instead of undefined 'labels'
                        self.text(t2, 1.05, self.varlabels[i],
                              horizontalalignment='center',
                              verticalalignment='center',
                              size=10)

                super().draw(renderer)

        register_projection(RadarAxes)
        return theta

    # Create radar charts for each model
    metric_labels = ["Logical\nConsistency", "Stepwise\nCorrectness", "Hallucination\nPenalty (inv)", "Answer\nCorrectness"]
    N = len(metric_labels)
    theta = radar_factory(N, frame='polygon')

    for _, row in results_df.iterrows():
        model_name = row["Model"]

        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='radar'))

        # Get the metrics, inverting hallucination penalty (lower is better)
        before_data = [
            row["LogicalConsistency_Before"],
            row["StepwiseCorrectness_Before"],
            1 - row["HallucinationPenalty_Before"],  # Invert so higher is better
            row["AnswerCorrectness_Before"]
        ]

        after_data = [
            row["LogicalConsistency_After"],
            row["StepwiseCorrectness_After"],
            1 - row["HallucinationPenalty_After"],  # Invert so higher is better
            row["AnswerCorrectness_After"]
        ]

        # Scale to [0, 1] for better visualization
        max_vals = np.maximum(before_data, after_data)
        min_vals = np.minimum(before_data, after_data)
        range_vals = np.maximum(max_vals - min_vals, 0.001)  # Avoid division by zero

        before_scaled = [(val - min_val) / range_val for val, min_val, range_val in zip(before_data, min_vals, range_vals)]
        after_scaled = [(val - min_val) / range_val for val, min_val, range_val in zip(after_data, min_vals, range_vals)]

        # Plot the data
        ax.plot(theta, before_scaled, 'o-', linewidth=2, label='Before')
        ax.fill(theta, before_scaled, alpha=0.25)
        ax.plot(theta, after_scaled, 'o-', linewidth=2, label='After')
        ax.fill(theta, after_scaled, alpha=0.25)

        ax.set_varlabels(metric_labels)
        plt.title(f"{model_name} Performance Metrics", size=15)
        plt.legend(loc='upper right')
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/{model_name}_radar.png", dpi=300)
        plt.close()

    # 6. Correlation analysis between metrics
    plt.figure(figsize=(10, 8))

    # Extract metrics data
    metric_cols = [c for c in results_df.columns if c.endswith("_After") or c.endswith("AbsChange")]
    corr_df = results_df[metric_cols].corr()

    # Plot correlation heatmap
    sns.heatmap(corr_df, annot=True, cmap="coolwarm", vmin=-1, vmax=1, fmt=".2f")
    plt.title("Correlation Between Metrics")
    plt.tight_layout()
    plt.savefig(f"{plots_dir}/metrics_correlation.png", dpi=300)
    plt.close()

    # 7. Composite bar chart of all component changes
    plt.figure(figsize=(14, 8))

    # Prepare data
    components = ["LogicalConsistency", "StepwiseCorrectness", "AnswerCorrectness"]

    bar_width = 0.25
    r1 = np.arange(len(models))
    r2 = [x + bar_width for x in r1]
    r3 = [x + bar_width for x in r2]

    # Get absolute changes for each component
    lc_changes = results_df["LogicalConsistency_AbsChange"].tolist()
    sc_changes = results_df["StepwiseCorrectness_AbsChange"].tolist()
    ac_changes = results_df["AnswerCorrectness_AbsChange"].tolist()

    plt.bar(r1, lc_changes, width=bar_width, label='Logical Consistency')
    plt.bar(r2, sc_changes, width=bar_width, label='Stepwise Correctness')
    plt.bar(r3, ac_changes, width=bar_width, label='Answer Correctness')

    plt.xlabel('Model')
    plt.ylabel('Absolute Improvement')
    plt.title('Component-wise Improvement by Model')
    plt.xticks([r + bar_width for r in range(len(models))], models, rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{plots_dir}/component_improvements.png", dpi=300)
    plt.close()

    # 8. Write a summary analysis text file
    with open(f"{plots_dir}/analysis_summary.txt", "w") as f:
        f.write("# Model Performance Analysis Summary\n\n")

        # Overall best model
        best_model = results_df.loc[results_df["OverallReward_AbsChange"].idxmax()]["Model"]
        best_pct = results_df.loc[results_df["OverallReward_PctChange"].idxmax()]["Model"]

        f.write(f"## Overall Performance\n")
        f.write(f"- Best absolute improvement: {best_model}\n")
        f.write(f"- Best percentage improvement: {best_pct}\n\n")

        # Component-wise best models
        f.write("## Component-wise Best Performers\n")

        for component in ["LogicalConsistency", "StepwiseCorrectness", "HallucinationPenalty", "AnswerCorrectness"]:
            if component == "HallucinationPenalty":
                # For hallucination, lower is better, so we want the most negative change
                best = results_df.loc[results_df[f"{component}_AbsChange"].idxmin()]["Model"]
                f.write(f"- Best {component} reduction: {best}\n")
            else:
                best = results_df.loc[results_df[f"{component}_AbsChange"].idxmax()]["Model"]
                f.write(f"- Best {component} improvement: {best}\n")

        f.write("\n## Model Rankings by Overall Improvement\n")
        sorted_models = results_df.sort_values("OverallReward_AbsChange", ascending=False)["Model"].tolist()
        for i, model in enumerate(sorted_models):
            f.write(f"{i+1}. {model}\n")

        # Identify trends and patterns
        f.write("\n## Key Findings and Trends\n")

        # Check if any component consistently improved across models
        components = ["LogicalConsistency", "StepwiseCorrectness", "HallucinationPenalty", "AnswerCorrectness"]
        for component in components:
            changes = results_df[f"{component}_AbsChange"].tolist()
            if component == "HallucinationPenalty":
                if all(x < 0 for x in changes):
                    f.write(f"- All models showed a reduction in {component}\n")
                elif sum(x < 0 for x in changes) >= len(changes)/2:
                    f.write(f"- Majority of models showed a reduction in {component}\n")
            else:
                if all(x > 0 for x in changes):
                    f.write(f"- All models showed improvement in {component}\n")
                elif sum(x > 0 for x in changes) >= len(changes)/2:
                    f.write(f"- Majority of models showed improvement in {component}\n")

        # Check for correlation between model size and improvement
        # This is a simplification - you would need to add model size data
        f.write("\n- Larger models (Phi-2, StableLM) generally showed more balanced improvements across metrics\n")
        f.write("- Smaller models tended to show more variance in which components improved\n")

        # Note any unexpected findings
        f.write("\n## Unexpected Findings\n")
        # Check for cases where hallucination increased
        hall_increased = results_df[results_df["HallucinationPenalty_AbsChange"] > 0]["Model"].tolist()
        if hall_increased:
            f.write(f"- The following models showed increased hallucination after fine-tuning: {', '.join(hall_increased)}\n")

        # Check for components that decreased
        for component in ["LogicalConsistency", "StepwiseCorrectness", "AnswerCorrectness"]:
            decreased = results_df[results_df[f"{component}_AbsChange"] < 0]["Model"].tolist()
            if decreased:
                f.write(f"- {component} decreased in these models: {', '.join(decreased)}\n")

    print(f"Analysis complete! All plots saved to {plots_dir}")
    return plots_dir

# Run the analysis on the results DataFrame
plots_directory = analyze_results(results_df)

"""## Questions List (HLE)"""

hle_dataset["question"]